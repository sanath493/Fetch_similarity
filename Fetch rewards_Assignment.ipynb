{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook provides the code and instructions to calculate the similarity between 2 texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Documents that need to be processed should be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\",\n",
    "             \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\",\n",
    "            \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stopwords data from a custom text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"stopwords.txt\", \"r\")\n",
    "stopwords=[]\n",
    "for line in f.readlines():\n",
    "    stopwords.append(line[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is as follows:\n",
    "\n",
    "1) We first convert all the words in each document into lower case and split them by space.\n",
    "\n",
    "2) We then check for the valid word format using a regex condition.(This is called stemming)\n",
    "\n",
    "3) We can the lemmatize the words i.e, get the core word(ex: doing --> do, helpful -->help etc).Currently this is not implemented as it needs a model to be trained.\n",
    "\n",
    "4) Then we filter out the lists obtained in step 3 using the custom stopwords(like a ,an,the ,you etc) list to get the meaningful words out.\n",
    "\n",
    "5) Then we create a corpus of all the unique words from all the documents(This will be our master corpus).\n",
    "\n",
    "6) For each word in the unique data we check for each sentence how many times the word gets repeated in it.\n",
    "\n",
    "7) This gives us the term frequency list(putting in dictionary)\n",
    "\n",
    "8) We them calculate the Inverse document frequency for each word in the unique data list.\n",
    "\n",
    "9) This can be done by checking how in how many documents a particular word appears and dividing the number of documents by the word appearence number.\n",
    "\n",
    "10) Multiplying the two values will give us the TF-IDF value for each document.\n",
    "\n",
    "11) We can then use this to calculate the cosine similarity between 2 vectors.\n",
    "\n",
    "12) Cosine similarity can be calculated by taking the (dot product of 2 vectors)/product of mod of the vectors\n",
    "\n",
    "13) This gives us a score(numerical value between 0 to 1) on how similar 2 texts are.\n",
    "\n",
    "14) If the queried texts are more similar the score is nearer to 1 and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming \n",
    "def Stemmer(doc):\n",
    "    stemmed_res = re.sub(r'[^\\w\\s]', '', doc).lower().split()\n",
    "    return stemmed_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Filtering out the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSentences(docs):\n",
    "    filter_sentences=[]\n",
    "    for document in docs:\n",
    "        filtered_doc=[w for w in Stemmer(document) if not w in stopwords] \n",
    "        filter_sentences.append(filtered_doc)\n",
    "    return filter_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Finding the number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numofWords(inp,distinct_words):\n",
    "    Words = dict.fromkeys(distinct_words, 0)\n",
    "    for word in inp:\n",
    "        Words[word] += 1\n",
    "    return Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numofWordsList(dist_word_list,distinct_words):\n",
    "    word_num_list=[]\n",
    "    for word_list in dist_word_list:\n",
    "        word_num_list.append(numofWords(word_list,distinct_words))\n",
    "    return word_num_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Computing the Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTf(wrd_cnt,total_words ):\n",
    "    tfdict = {}\n",
    "    N = len(total_words)\n",
    "    for word, count in wrd_cnt.items():\n",
    "        tfdict[word] = count / float(N)\n",
    "    return tfdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Computing the Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findIdf(sentences_list,dist_words):\n",
    "    dict_total={}\n",
    "    n=len(sentences_list)\n",
    "    for i in dist_words:\n",
    "        for each in sentences_list:\n",
    "            if i in each:\n",
    "                if i in dict_total:\n",
    "                    dict_total[i]=dict_total[i]+1\n",
    "                else:\n",
    "                    dict_total[i]=1 \n",
    "    for i in dict_total:\n",
    "        dict_total[i]=math.log(n+1/dict_total[i])\n",
    "    return dict_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTFIDF(words, idf_val,distinct_words):\n",
    "    tfidf = {}\n",
    "    for word in distinct_words:\n",
    "        tfidf[word] = words[word] * idf_val[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support functions to find sum and modulus of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotProduct(l1,l2):\n",
    "    sum_final=0\n",
    "    for i in range(len(l1)):\n",
    "        sum_final=sum_final+(l1[i]*l2[i])\n",
    "    return sum_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulus(vec):\n",
    "    sum_of_sq=0\n",
    "    for i in vec:\n",
    "        sum_of_sq=sum_of_sq+(i*i)\n",
    "    return numpy.sqrt(sum_of_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to find the similarity between 2 documents(As the numbering starts from 0, please give the document ids starting from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarity(doc1_id,doc2_id,documnets):\n",
    "    #Filtering out the stop words\n",
    "    filtered_document_list=filterSentences(documents)\n",
    "    #Finding the distinct set of words\n",
    "    distinct_words= set().union(*filtered_document_list)\n",
    "    #Finding the number of words\n",
    "    word_count=numofWordsList(filtered_document_list,distinct_words)\n",
    "    #Computing the Term Frequency\n",
    "    tfs=[]\n",
    "    for i in range(len(word_count)):\n",
    "        tfs.append(findTf(word_count[i],filtered_document_list[i]))\n",
    "    #Computing the Inverse document frequency\n",
    "    idf_val=findIdf(filtered_document_list,distinct_words)\n",
    "    doc1=list(findTFIDF(tfs[doc1_id],idf_val,distinct_words).values())\n",
    "    doc2=list(findTFIDF(tfs[doc2_id],idf_val,distinct_words).values())\n",
    "    #finding dot product\n",
    "    dot_product=dotProduct(doc1,doc2)\n",
    "    #finding modulud product\n",
    "    deno=modulus(doc1)*modulus(doc2)\n",
    "    #finding cosine similarity\n",
    "    cosine_similarity=dot_product/deno\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to find the similarities between all the documents (gives 2d array) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDocSimilarity(documents):\n",
    "    final_arr=[]\n",
    "    n=len(documents)\n",
    "    for i in range(n):\n",
    "        s1=[]\n",
    "        for j in range(n):\n",
    "            val=round(findSimilarity(i,j,documents),3)\n",
    "            s1.append(val)\n",
    "        final_arr.append(s1)\n",
    "    return final_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 2d array we can check the similarity value of 2 sentences by referring to the matrix rows and columns accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.753, 0.274], [0.753, 1.0, 0.238], [0.274, 0.238, 1.0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output=findDocSimilarity(documents)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose if we need the similarity score of first and second sentence we check row 1 column 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose if we need the similarity score of first and third sentence we check row 2 column 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.238"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be used directly to find similarity between 2 sentences directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSentenceSimilarity(doc1,doc2):\n",
    "    documents=[doc1,doc2]\n",
    "    filtered_document_list=filterSentences(documents)\n",
    "    distinct_words= set().union(*filtered_document_list)\n",
    "    word_count=numofWordsList(filtered_document_list,distinct_words)\n",
    "    tfs=[]\n",
    "    for i in range(len(word_count)):\n",
    "        tfs.append(findTf(word_count[i],filtered_document_list[i]))\n",
    "    idf_val=findIdf(filtered_document_list,distinct_words)\n",
    "    doc1=list(findTFIDF(tfs[0],idf_val,distinct_words).values())\n",
    "    doc2=list(findTFIDF(tfs[1],idf_val,distinct_words).values())\n",
    "    dot_product=dotProduct(doc1,doc2)\n",
    "    deno=modulus(doc1)*modulus(doc2)\n",
    "    cosine_similarity=dot_product/deno\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the score for first and second sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7259076475745524"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findSentenceSimilarity(\"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\",\n",
    "             \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the score for second and third sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24058207580417887"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findSentenceSimilarity(\"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\",\n",
    "            \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of approach one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 (extra) using library models (Ignore this if necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is using the word2vec embedding model using gensim. We can always train a custom model for word 2 vec,but here we use gensim model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "filtered_document_list=filterSentences(documents)\n",
    "model = gensim.models.Word2Vec(filtered_document_list, size=150, window=10, min_count=1, workers=10, iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the similarity using the trained model\n",
    "\n",
    "document[0] is sentence 1\n",
    "\n",
    "document[0] is sentence 2\n",
    "\n",
    "document[0] is sentence 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarity(doc1,doc2):\n",
    "    s1 = doc1\n",
    "    s2 = doc2 \n",
    "    s1=[w for w in Stemmer(s1) if not w in stopwords] \n",
    "    s2=[w for w in Stemmer(s2) if not w in stopwords] \n",
    "    similarity = model.wv.n_similarity(s1, s2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between document 1 and document 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84225047"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findSimilarity(documents[0],documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between document 1 and document 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3812753"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findSimilarity(documents[0],documents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
